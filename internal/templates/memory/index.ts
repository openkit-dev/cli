// Semantic Memory Plugin for OpenCode
// Auto-generated by OpenKit
//
// This plugin automatically captures and persists context across agent sessions
// using local vector storage (LanceDB) and ONNX embeddings.

import type { Plugin } from "@opencode-ai/plugin"
import { tool } from "@opencode-ai/plugin/tool"
import { SemanticMemory } from "./lib/memory.ts"
import * as fs from "fs/promises"

// Session metrics for debugging
interface SessionMetrics {
  sessionId: string
  startTime: number
  memoriesLoaded: number
  memoriesInjected: number
  tokensInjected: number
  compactionTriggered: boolean
  extractionTriggered: boolean
}

// Rough token estimation (1 token ~ 4 chars)
function estimateTokens(text: string): number {
  return Math.ceil(text.length / 4)
}

export const MemoryPlugin: Plugin = async (ctx) => {
  const configPath = `${ctx.worktree}/.opencode/memory/config.json`
  const dbPath = `${ctx.worktree}/.opencode/memory/index.lance`
  const metricsPath = `${ctx.worktree}/.opencode/memory/metrics.json`
  
  const memory = new SemanticMemory({ configPath, dbPath })
  let config: any = {}
  let currentMetrics: SessionMetrics | null = null
  
  try {
    await memory.initialize()
    // Load config for debug settings
    try {
      const configContent = await fs.readFile(configPath, 'utf-8')
      config = JSON.parse(configContent)
    } catch {
      config = { debug: { verbose: false, show_injection_indicator: true } }
    }
  } catch (err) {
    console.error("[semantic-memory] Failed to initialize:", err)
    return {}
  }

  // Helper to log metrics
  async function saveMetrics(metrics: SessionMetrics) {
    try {
      let allMetrics: SessionMetrics[] = []
      try {
        const existing = await fs.readFile(metricsPath, 'utf-8')
        allMetrics = JSON.parse(existing)
      } catch {}
      allMetrics.push(metrics)
      if (allMetrics.length > 100) {
        allMetrics = allMetrics.slice(-100)
      }
      await fs.writeFile(metricsPath, JSON.stringify(allMetrics, null, 2))
    } catch (err) {
      if (config.debug?.verbose) {
        console.error("[semantic-memory] Failed to save metrics:", err)
      }
    }
  }

  return {
    // Load relevant context at session start
    "session.created": async (input, output) => {
      try {
        currentMetrics = {
          sessionId: input.sessionId,
          startTime: Date.now(),
          memoriesLoaded: 0,
          memoriesInjected: 0,
          tokensInjected: 0,
          compactionTriggered: false,
          extractionTriggered: false
        }

        const query = input.initialMessage || input.metadata?.task || "project context"
        const relevant = await memory.getRelevantContext(query, 10)
        memory.setSessionCache(relevant)
        
        currentMetrics.memoriesLoaded = relevant.length
        
        if (relevant.length > 0 && config.debug?.show_toast_on_load !== false) {
          try {
            const tokensLoaded = relevant.reduce((sum, m) => sum + estimateTokens(m.content), 0)
            await ctx.client?.tui?.showToast?.({
              body: {
                message: `Memory: ${relevant.length} memories loaded (~${tokensLoaded} tokens)`,
                variant: "info"
              }
            })
          } catch {}
        }
        
        if (config.debug?.verbose) {
          console.log(`[semantic-memory] Session ${input.sessionId} started, loaded ${relevant.length} memories`)
        }
      } catch (err) {
        console.error("[semantic-memory] session.created error:", err)
      }
    },

    // Inject memory into compaction prompt
    "experimental.session.compacting": async (input, output) => {
      try {
        const memories = memory.getSessionCache()
        
        if (currentMetrics) {
          currentMetrics.compactionTriggered = true
        }
        
        if (memories.length > 0) {
          const formatted = memories
            .map(m => `- [${m.type}] ${m.title}: ${m.content.slice(0, 500)}${m.content.length > 500 ? '...' : ''}`)
            .join('\n')
          
          const tokensUsed = estimateTokens(formatted)
          
          if (currentMetrics) {
            currentMetrics.memoriesInjected = memories.length
            currentMetrics.tokensInjected = tokensUsed
          }
          
          let contextBlock = ''
          if (config.debug?.show_injection_indicator) {
            contextBlock += `<!-- [SEMANTIC-MEMORY] Injected ${memories.length} memories (~${tokensUsed} tokens) -->\n`
          }
          contextBlock += `## Project Memory (Optimized Context)\n\n`
          contextBlock += `> ${memories.length} memories | ~${tokensUsed} tokens\n\n`
          contextBlock += formatted
          
          output.context.push(contextBlock)
          
          if (config.debug?.show_toast_on_inject !== false) {
            try {
              await ctx.client?.tui?.showToast?.({
                body: {
                  message: `Memory injected: ${memories.length} memories (~${tokensUsed} tokens)`,
                  variant: "success"
                }
              })
            } catch {}
          }
        }
      } catch (err) {
        console.error("[semantic-memory] session.compacting error:", err)
      }
    },

    // Extract knowledge when session ends
    "session.idle": async (input) => {
      try {
        if (currentMetrics) {
          currentMetrics.extractionTriggered = true
        }
        
        await memory.extractFromSession(input.sessionId, ctx.client)
        
        if (currentMetrics) {
          await saveMetrics(currentMetrics)
        }
      } catch (err) {
        console.error("[semantic-memory] session.idle error:", err)
      }
    },

    // Custom tools using the correct OpenCode format
    tool: {
      memory_query: tool({
        description: "Search the semantic memory system for relevant context. This is a built-in tool - use it directly without checking if the plugin exists.",
        args: {
          query: tool.schema.string().describe("Search query text"),
          limit: tool.schema.number().optional().describe("Max results (default: 5)")
        },
        async execute(args, context) {
          try {
            const results = await memory.search(args.query, args.limit || 5)
            if (results.length === 0) {
              return "No relevant memories found for this query."
            }
            const formatted = results.map(r => 
              `[${r.type}] ${r.title}\n  Content: ${r.content.slice(0, 300)}...\n  Salience: ${r.salience.toFixed(2)}`
            ).join('\n\n')
            return `Found ${results.length} relevant memories:\n\n${formatted}`
          } catch (err: any) {
            return `Error querying memory: ${err.message}`
          }
        }
      }),

      memory_stats: tool({
        description: "Get statistics about stored memories (count, types, tokens). This is a built-in tool - use it directly without verification.",
        args: {},
        async execute(args, context) {
          try {
            const allMemories = await memory.getAllMemories(1000)
            
            if (allMemories.length === 0) {
              return "No memories stored yet. Memories are automatically extracted from your sessions."
            }

            const byType: Record<string, number> = {}
            let totalTokens = 0
            
            for (const m of allMemories) {
              byType[m.type] = (byType[m.type] || 0) + 1
              totalTokens += estimateTokens(m.content)
            }

            const typeBreakdown = Object.entries(byType)
              .map(([type, count]) => `  - ${type}: ${count}`)
              .join('\n')

            return `Semantic Memory Statistics:

Total Memories: ${allMemories.length}
Total Tokens Stored: ~${totalTokens}

By Type:
${typeBreakdown}

Configuration:
  - Token budget: ${config.retrieval?.token_budget || 4000}
  - Max results: ${config.retrieval?.max_results || 10}
  - Min similarity: ${config.retrieval?.min_similarity || 0.7}`
          } catch (err: any) {
            return `Error getting stats: ${err.message}`
          }
        }
      }),

      memory_save: tool({
        description: "Save a decision, pattern, or important context to semantic memory. This is a built-in tool - use it directly without verification.",
        args: {
          type: tool.schema.string().describe("Type: decision, pattern, error, spec, or context"),
          title: tool.schema.string().describe("Short title for this memory"),
          content: tool.schema.string().describe("Full content to remember")
        },
        async execute(args, context) {
          try {
            const validTypes = ['decision', 'pattern', 'error', 'spec', 'context'] as const
            const type = validTypes.includes(args.type as any) ? args.type as typeof validTypes[number] : 'context'
            
            await memory.createMemory({
              type,
              title: args.title,
              content: args.content,
              files: []
            })

            // Simple single-line return to avoid TUI display issues
            return `Saved ${type} memory: "${args.title}" (~${estimateTokens(args.content)} tokens)`
          } catch (err: any) {
            return `Error saving memory: ${err.message}`
          }
        }
      }),

      memory_debug: tool({
        description: "Show debug status of the semantic memory system. This is a built-in tool - use it directly.",
        args: {},
        async execute(args, context) {
          try {
            const cached = memory.getSessionCache()
            return `Semantic Memory Debug Status:

Plugin Status: ACTIVE
Session Cache: ${cached.length} memories loaded
Compaction Hook: ${currentMetrics?.compactionTriggered ? 'TRIGGERED' : 'NOT YET'}
Extraction Hook: ${currentMetrics?.extractionTriggered ? 'TRIGGERED' : 'NOT YET'}

Current Session:
  - Memories loaded: ${currentMetrics?.memoriesLoaded || 0}
  - Memories injected: ${currentMetrics?.memoriesInjected || 0}
  - Tokens injected: ~${currentMetrics?.tokensInjected || 0}`
          } catch (err: any) {
            return `Debug error: ${err.message}`
          }
        }
      })
    }
  }
}
